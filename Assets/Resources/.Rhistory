specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
library(caret)
# We copy the code from the first model and use cp = .005.
rfit <- rpart(WL ~ .,data = white_train_data,method = "class",cp = .005)
plot(rfit,margin = .2)
text(rfit,cex = .75,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
library(caret)
# We can't use my code which works for the binary situation anymore. Instead,
# we use a function in the caret package designed for computing metrics.
confusionMatrix(rpredict,as.factor(white_test_data$WL))
# We can experiment with the cp parameter to see if we can improve results.
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(readr)
library(dslabs)
library(sjstats)
library(kableExtra)
library(pander)
library(rpart)
library(caret)
White_Heuristic_Weights <- read.csv("WhiteHeuristicWeightsTable.csv")
Black_Heuristic_Weights <- read.csv("BlackHeuristicWeightsTable.csv")
White_Heuristic_Weights$WL <- as.factor(White_Heuristic_Weights$WL)
Black_Heuristic_Weights$WL <- as.factor(Black_Heuristic_Weights$WL)
str(White_Heuristic_Weights)
str(Black_Heuristic_Weights)
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .2)
text(rfit,cex = .75,use.n = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(readr)
library(dslabs)
library(sjstats)
library(kableExtra)
library(pander)
library(rpart)
library(caret)
White_Heuristic_Weights <- read.csv("WhiteHeuristicWeightsTable.csv")
Black_Heuristic_Weights <- read.csv("BlackHeuristicWeightsTable.csv")
White_Heuristic_Weights$WL <- as.factor(White_Heuristic_Weights$WL)
Black_Heuristic_Weights$WL <- as.factor(Black_Heuristic_Weights$WL)
str(White_Heuristic_Weights)
str(Black_Heuristic_Weights)
# Separate data into training data and testing data
index1 <- sample(1:1171,900,replace = FALSE)
white_test_data <- White_Heuristic_Weights[-index1,]
white_train_data <- White_Heuristic_Weights[index1,]
index2 <- sample(1:1170,900,replace = FALSE)
black_test_data <- Black_Heuristic_Weights[-index2,]
black_train_data <- Black_Heuristic_Weights[index2,]
#################### Histograms of Predictors ##############
# We use this code when we want to look at several predictors
# to see if they have good "normal" variation.
White_Heuristic_Weights %>%
gather(-WL, key = "var", value = "value") %>%
ggplot(aes(x = value, color = "lightblue")) +
geom_histogram(bins=20,color = "black",fill = "lightblue") +
facet_wrap(~ var, scales = "free") +
labs(title = "Distributions of Predictors - White") +
theme(axis.title.x=element_blank(),axis.title.y = element_blank(), axis.text.x = element_blank())
############################################################
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .2)
text(rfit,cex = .75,use.n = TRUE)
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .2)
text(rfit,cex = .75,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .2)
text(rfit,cex = .2,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .5)
text(rfit,cex = .2,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .1)
text(rfit,cex = .2,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .9)
text(rfit,cex = .2,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .01)
text(rfit,cex = .2,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .01)
text(rfit,cex = .4,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .01)
text(rfit,cex = .3,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(readr)
library(dslabs)
library(sjstats)
library(kableExtra)
library(pander)
library(rpart)
library(caret)
White_Heuristic_Weights <- read.csv("WhiteHeuristicWeightsTable.csv")
Black_Heuristic_Weights <- read.csv("BlackHeuristicWeightsTable.csv")
White_Heuristic_Weights$WL <- as.factor(White_Heuristic_Weights$WL)
Black_Heuristic_Weights$WL <- as.factor(Black_Heuristic_Weights$WL)
str(White_Heuristic_Weights)
str(Black_Heuristic_Weights)
# Separate data into training data and testing data
index1 <- sample(1:1171,900,replace = FALSE)
white_test_data <- White_Heuristic_Weights[-index1,]
white_train_data <- White_Heuristic_Weights[index1,]
index2 <- sample(1:1170,900,replace = FALSE)
black_test_data <- Black_Heuristic_Weights[-index2,]
black_train_data <- Black_Heuristic_Weights[index2,]
#################### Histograms of Predictors ##############
# We use this code when we want to look at several predictors
# to see if they have good "normal" variation.
White_Heuristic_Weights %>%
gather(-WL, key = "var", value = "value") %>%
ggplot(aes(x = value, color = "lightblue")) +
geom_histogram(bins=20,color = "black",fill = "lightblue") +
facet_wrap(~ var, scales = "free") +
labs(title = "Distributions of Predictors - White") +
theme(axis.title.x=element_blank(),axis.title.y = element_blank(), axis.text.x = element_blank())
############################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .01)
text(rfit,cex = .35,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We copy the code from the first model and use cp = .005.
rfit <- rpart(WL ~ .,data = white_train_data,method = "class",cp = .005)
plot(rfit,margin = .01)
text(rfit,cex = .35,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
# We can't use my code which works for the binary situation anymore. Instead,
# we use a function in the caret package designed for computing metrics.
confusionMatrix(rpredict,as.factor(white_test_data$WL))
# We can experiment with the cp parameter to see if we can improve results.
#################################################################################################
# Try a logistic model with **ALL PREDICTORS**
glm_fit <- white_train_data %>% glm(WL ~ .,
data =.,family = "binomial")
p_hat_glm <- predict(glm_fit,white_test_data, type = "response")
y_hat_glm <- factor(ifelse(p_hat_glm > .5,1,0),levels = c("0","1"))
T <- table(white_test_data$WL,y_hat_glm)
# Copy metric code:
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
Tmetrics <- data.frame(Metric = metric,Value = round(value,3)) #
knitr::kable(Tmetrics)                                         #
################################################################
# Acc = .809, Sens = .812, Spec = .807 AVG = .8093
# We seek to remove variables in order to obtain the best model
# possible utilizing logistic regression, backwards elimination.
summary(glm_fit)
#################################################################################################
# Try a logistic model with **ALL PREDICTORS**
glm_fit <- black_train_data %>% glm(WL ~ .,
data =.,family = "binomial")
p_hat_glm <- predict(glm_fit,black_test_data, type = "response")
y_hat_glm <- factor(ifelse(p_hat_glm > .5,1,0),levels = c("0","1"))
T <- table(black_test_data$WL,y_hat_glm)
# Copy metric code:
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
Tmetrics <- data.frame(Metric = metric,Value = round(value,3)) #
knitr::kable(Tmetrics)                                         #
################################################################
# Acc = .809, Sens = .812, Spec = .807 AVG = .8093
# We seek to remove variables in order to obtain the best model
# possible utilizing logistic regression, backwards elimination.
summary(glm_fit)
model1 <- lm(WL ~ .,data = white_train_data)
summary(model1)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(knitr)
library(readr)
library(dslabs)
library(sjstats)
library(kableExtra)
library(pander)
library(rpart)
library(caret)
White_Heuristic_Weights <- read.csv("WhiteHeuristicWeightsTable.csv")
Black_Heuristic_Weights <- read.csv("BlackHeuristicWeightsTable.csv")
White_Heuristic_Weights$WL <- as.factor(White_Heuristic_Weights$WL)
Black_Heuristic_Weights$WL <- as.factor(Black_Heuristic_Weights$WL)
str(White_Heuristic_Weights)
str(Black_Heuristic_Weights)
# Separate data into training data and testing data
index1 <- sample(1:1171,900,replace = FALSE)
white_test_data <- White_Heuristic_Weights[-index1,]
white_train_data <- White_Heuristic_Weights[index1,]
index2 <- sample(1:1170,900,replace = FALSE)
black_test_data <- Black_Heuristic_Weights[-index2,]
black_train_data <- Black_Heuristic_Weights[index2,]
#################### Histograms of Predictors ##############
# We use this code when we want to look at several predictors
# to see if they have good "normal" variation.
White_Heuristic_Weights %>%
gather(-WL, key = "var", value = "value") %>%
ggplot(aes(x = value, color = "lightblue")) +
geom_histogram(bins=20,color = "black",fill = "lightblue") +
facet_wrap(~ var, scales = "free") +
labs(title = "Distributions of Predictors - White") +
theme(axis.title.x=element_blank(),axis.title.y = element_blank(), axis.text.x = element_blank())
############################################################
# We fit a classification tree to the data using every variable. The goal is
# to predict whether the heuristics equal W or L
rfit <- rpart(WL ~ .,data = white_train_data,method = "class")
plot(rfit,margin = .01)
text(rfit,cex = .35,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
T <- table(rpredict,white_test_data$WL)
# Copy metrics
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
data.frame(Metric = metric,Value = round(value,3))             #
################################################################
# We copy the code from the first model and use cp = .005.
rfit <- rpart(WL ~ .,data = white_train_data,method = "class",cp = .005)
plot(rfit,margin = .01)
text(rfit,cex = .35,use.n = TRUE)
rpredict <- predict(rfit,white_test_data,type = "class")
# We can't use my code which works for the binary situation anymore. Instead,
# we use a function in the caret package designed for computing metrics.
confusionMatrix(rpredict,as.factor(white_test_data$WL))
# We can experiment with the cp parameter to see if we can improve results.
#################################################################################################
# Try a logistic model with **ALL PREDICTORS**
glm_fit <- white_train_data %>% glm(WL ~ .,
data =.,family = "binomial")
p_hat_glm <- predict(glm_fit,white_test_data, type = "response")
y_hat_glm <- factor(ifelse(p_hat_glm > .5,1,0),levels = c("0","1"))
T <- table(white_test_data$WL,y_hat_glm)
# Copy metric code:
############################ Metrics for Binary Classification #
T <- as.vector(T)                                              #
# Accuracy  is (num. correctly predicted)/(total)              #
accuracy <- (T[1]+T[4])/(T[1]+T[2]+T[3]+T[4]) # correct/total  #
# Sensitivity is Pr(predict 1 given actually 1) =              #
sensitivity <- T[4]/(T[3]+T[4])                                #
# Specificity is Pr(predict 0 given actually 0) =              #
specificity <- T[1]/(T[1]+T[2])                                #
metric <- c("Accuracy","Sensitivity","Specificity")            #
value <- c(accuracy,sensitivity,specificity)                   #
Tmetrics <- data.frame(Metric = metric,Value = round(value,3)) #
knitr::kable(Tmetrics)                                         #
################################################################
# Acc = .809, Sens = .812, Spec = .807 AVG = .8093
# We seek to remove variables in order to obtain the best model
# possible utilizing logistic regression, backwards elimination.
summary(glm_fit)
model1 <- lm(WL ~ .,data = white_train_data)
summary(model1)
model1 <- lm(WL ~ .,data = white_train_data)
#summary(model1)
rmse(model1)
# This is an excellent model RMSE = .6 ish (I've seen .58)
# We seek to validate this model with a t-test
# First make predictions:
predicted_values <- predict(model1,white_test_data)
# Visualize the prediction validity (basic plot):
plot(white_test_data$WL,predicted_values,
xlim = c(0,4),ylim = c(0,4))
abline(a = 0, b =1,col="red")
# This makes me wonder about hs_gpa vs fy_gpa
#plot(test_data$hs_gpa,test_data$fy_gpa,
#     xlim = c(0,4),ylim = c(0,4))
#abline(a = 0, b =1,col="red")
# t-test for difference between fy GPA and what we predict:
# Null hypothesis is the difference is zero.
t.test(test_data$WL - predicted_values)
model1 <- lm(WL ~ .,data = white_train_data)
#summary(model1)
rmse(model1)
# This is an excellent model RMSE = .6 ish (I've seen .58)
# We seek to validate this model with a t-test
# First make predictions:
predicted_values <- predict(model1,white_test_data)
# Visualize the prediction validity (basic plot):
plot(white_test_data$WL,predicted_values,
xlim = c(0,4),ylim = c(0,4))
abline(a = 0, b =1,col="red")
# This makes me wonder about hs_gpa vs fy_gpa
#plot(test_data$hs_gpa,test_data$fy_gpa,
#     xlim = c(0,4),ylim = c(0,4))
#abline(a = 0, b =1,col="red")
# t-test for difference between fy GPA and what we predict:
# Null hypothesis is the difference is zero.
t.test(white_test_data$WL - predicted_values)
model1 <- lm(WL ~ .,data = white_train_data)
#summary(model1)
rmse(model1)
# We seek to validate this model with a t-test
# First make predictions:
predicted_values <- predict(model1,white_test_data)
# Visualize the prediction validity (basic plot):
plot(white_test_data$WL,predicted_values,
xlim = c(0,4),ylim = c(0,4))
abline(a = 0, b =1,col="red")
# Visualize the prediction validity (basic plot):
plot(white_test_data$WL,predicted_values,
xlim = c(0,4),ylim = c(0,4))
abline(a = 0, b =1,col="red")
model1 <- lm(WL ~ .,data = white_train_data)
#summary(model1)
rmse(model1)
# This is an excellent model RMSE = .6 ish (I've seen .58)
# We seek to validate this model with a t-test
# First make predictions:
predicted_values <- predict(model1,white_test_data)
# Visualize the prediction validity (basic plot):
plot(white_test_data$WL,predicted_values,
xlim = c(0,4),ylim = c(0,4))
abline(a = 0, b =1,col="red")
# This makes me wonder about hs_gpa vs fy_gpa
#plot(test_data$hs_gpa,test_data$fy_gpa,
#     xlim = c(0,4),ylim = c(0,4))
#abline(a = 0, b =1,col="red")
# t-test for difference between fy GPA and what we predict:
# Null hypothesis is the difference is zero.
#t.test(white_test_data$WL - predicted_values)
# This has the two indicators of an excellent
# validation: high p-val and tight confidence interval
# that includes zero.
